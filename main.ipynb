{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    BitsAndBytesConfig # Optional: for quantization\n",
    ")\n",
    "from peft import PeftModel, LoraConfig, get_peft_model, prepare_model_for_kbit_training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd89922",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca1060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "model_id = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "dataset_id = \"HuggingFaceTB/smoltalk\"\n",
    "output_dir = \"./smoltalk-finetuned-deepseek-lora\"\n",
    "\n",
    "# LoRA Configuration \n",
    "rank_dimension = 6\n",
    "lora_alpha = 8\n",
    "lora_dropout = 0.15\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Quantization Config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f85e4f",
   "metadata": {},
   "source": [
    "### Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a624fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Tokenizer missing pad token, setting to eos_token.\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "print(f\"Loading model: {model_id}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f276fa",
   "metadata": {},
   "source": [
    "### Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca329465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset into train (90%) and eval (10%)...\n",
      "Train dataset size: 2034\n",
      "Eval dataset size: 226\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id, \"everyday-conversations\", split=\"train\")\n",
    "\n",
    "# 10% for evaluation\n",
    "eval_split_percentage = 0.1\n",
    "\n",
    "# Split the dataset\n",
    "print(f\"Splitting dataset into train ({1-eval_split_percentage:.0%}) and eval ({eval_split_percentage:.0%})...\")\n",
    "split_dataset = dataset.train_test_split(test_size=eval_split_percentage, shuffle=True, seed=42) # Seed for reproducibility\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Eval dataset size: {len(eval_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc92c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust based on VRAM and expected conversation length\n",
    "max_seq_length = 1024\n",
    "\n",
    "# Define preprocessing function\n",
    "def preprocess_conversations(examples):\n",
    "    all_input_ids = []\n",
    "    all_attention_masks = []\n",
    "\n",
    "    for conversation in examples['messages']:\n",
    "        # Adds special tokens (e.g., <|im_start|>, <|im_end|>)\n",
    "        formatted_chat = tokenizer.apply_chat_template(\n",
    "            conversation,\n",
    "            tokenize=False, # Get the formatted string first\n",
    "            add_generation_prompt=False # We are training, not prompting\n",
    "        )\n",
    "\n",
    "        # Add the EOS token at the end of the conversation\n",
    "        if not formatted_chat.endswith(tokenizer.eos_token):\n",
    "             formatted_chat += tokenizer.eos_token\n",
    "\n",
    "        # Tokenize the fully formatted string\n",
    "        tokenized_output = tokenizer(\n",
    "            formatted_chat,\n",
    "            truncation=True,\n",
    "            max_length=max_seq_length,\n",
    "            padding=False # DataCollator will handle padding per batch\n",
    "        )\n",
    "\n",
    "        all_input_ids.append(tokenized_output['input_ids'])\n",
    "        all_attention_masks.append(tokenized_output['attention_mask'])\n",
    "\n",
    "    return {\"input_ids\": all_input_ids, \"attention_mask\": all_attention_masks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d405a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train dataset...\n",
      "Preprocessing eval dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226/226 [00:00<00:00, 1335.78 examples/s]\n",
      "Filter: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 226/226 [00:00<00:00, 5002.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Preprocess BOTH train and eval datasets\n",
    "print(\"Preprocessing train dataset...\")\n",
    "tokenized_train_dataset = train_dataset.map(\n",
    "    preprocess_conversations, batched=True, remove_columns=train_dataset.column_names\n",
    ").filter(lambda example: len(example['input_ids']) > 0)\n",
    "\n",
    "print(\"Preprocessing eval dataset...\")\n",
    "tokenized_eval_dataset = eval_dataset.map(\n",
    "    preprocess_conversations, batched=True, remove_columns=eval_dataset.column_names\n",
    ").filter(lambda example: len(example['input_ids']) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33862b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train dataset size: 2034\n",
      "Processed eval dataset size: 226\n",
      "Sample tokenized train input_ids: [151646, 151646, 151644, 13048, 151645, 9707, 0, 2585, 646, 358, 1492, 498, 3351, 30, 151643, 151644, 40, 2776, 6832, 911, 4285, 12645, 304, 2906, 323, 358, 572, 20293, 11, 1128, 374, 264, 27505, 30, 151645, 32, 27505, 374, 264, 4285, 5662, 429, 8609, 601, 11893, 476, 3271, 8811, 6171, 553, 1667, 264, 3619, 476, 23418, 429, 41330, 2412, 2163, 264, 8356, 1459, 11, 2598, 264, 5599, 5082, 372, 13, 151643, 151644, 4792, 3643, 5530, 13, 2055, 11, 1246, 1558, 432, 1281, 2513, 8661, 311, 11893, 30, 151645, 32, 27505, 3643, 2513, 8661, 311, 11893, 553, 10018, 279, 5106, 476, 3311, 315, 5344, 4362, 13, 1752, 3110, 11, 421, 498, 990, 264, 9276, 2257, 311, 11893, 264, 8811, 6946, 11, 279, 5599, 5082, 372, 374, 279, 1459, 1380, 279, 9276, 2257, 374, 40119, 389, 279, 4910, 11, 323, 279, 5344, 498, 3796, 311, 279, 1008, 835, 315, 279, 9276, 2257, 374, 8455, 1870, 11, 3259, 432, 8661, 311, 11893, 279, 6946, 13, 151643, 151644, 32313, 11, 358, 1744, 358, 633, 432, 1431, 13, 11114, 369, 25021, 432, 311, 752, 0, 151645, 2610, 2299, 10565, 0, 358, 2776, 15713, 358, 1410, 1492, 498, 3535, 512, 3004, 2664, 13, 151643]\n",
      "Decoded sample: <ï½œbeginâ–ofâ–sentenceï½œ><ï½œbeginâ–ofâ–sentenceï½œ><ï½œUserï½œ>Hi<ï½œAssistantï½œ>Hello! How can I help you today?<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>I'm learning about simple machines in school and I was wondering, what is a lever?<ï½œAssistantï½œ>A lever is a simple machine that helps us lift or move heavy objects by using a bar or beam that pivots around a fixed point, called a fulcrum.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>That makes sense. So, how does it make things easier to lift?<ï½œAssistantï½œ>A lever makes things easier to lift by changing the direction or amount of force needed. For example, if you use a crowbar to lift a heavy rock, the fulcrum is the point where the crowbar is resting on the ground, and the force you apply to the other end of the crowbar is magnified, making it easier to lift the rock.<ï½œendâ–ofâ–sentenceï½œ><ï½œUserï½œ>Okay, I think I get it now. Thanks for explaining it to me!<ï½œAssistantï½œ>You're welcome! I'm glad I could help you understand levers better.<ï½œendâ–ofâ–sentenceï½œ>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processed train dataset size: {len(tokenized_train_dataset)}\")\n",
    "print(f\"Processed eval dataset size: {len(tokenized_eval_dataset)}\")\n",
    "if len(tokenized_train_dataset) > 0:\n",
    "     print(f\"Sample tokenized train input_ids: {tokenized_train_dataset[0]['input_ids']}\")\n",
    "     print(f\"Decoded sample: {tokenizer.decode(tokenized_train_dataset[0]['input_ids'])}\")\n",
    "else:\n",
    "     print(\"Warning: Train dataset is empty after preprocessing!\")\n",
    "if len(tokenized_eval_dataset) == 0:\n",
    "    print(\"Warning: Eval dataset is empty after preprocessing! Evaluation may not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed73ebe",
   "metadata": {},
   "source": [
    "#### Apply PEFT (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de3903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying LoRA configuration...\n",
      "trainable params: 6,924,288 || all params: 1,784,012,288 || trainable%: 0.3881\n"
     ]
    }
   ],
   "source": [
    "# Prepare model for k-bit training if using quantization\n",
    "model = prepare_model_for_kbit_training(model) \n",
    "\n",
    "print(\"Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21804b08",
   "metadata": {},
   "source": [
    "#### Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb68a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability and BF16 support\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,      \n",
    "    gradient_accumulation_steps=8,     \n",
    "    learning_rate=2e-4,\n",
    "    num_train_epochs=20,      \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_ratio=0.03,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    # save_steps=100,\n",
    "    optim=\"paged_adamw_8bit\",          \n",
    "    bf16=use_bf16,\n",
    "    fp16=not use_bf16 and torch.cuda.is_available(),\n",
    "    tf32=use_bf16,\n",
    "    gradient_checkpointing=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c7941b4",
   "metadata": {},
   "source": [
    "#### Define Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d2725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pads batches dynamically and creates labels\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7e57ce",
   "metadata": {},
   "source": [
    "#### Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "850e8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_eval_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c12520",
   "metadata": {},
   "source": [
    "#### Start Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6747520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1270' max='1270' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1270/1270 1:33:55, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.036200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.370000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.114900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.053800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best LoRA adapter to ./smoltalk-finetuned-deepseek-lora\n",
      "***** train metrics *****\n",
      "  epoch                    =     9.9282\n",
      "  total_flos               = 32103354GF\n",
      "  train_loss               =     1.3729\n",
      "  train_perplexity         =     3.9468\n",
      "  train_runtime            = 1:34:00.59\n",
      "  train_samples_per_second =      3.606\n",
      "  train_steps_per_second   =      0.225\n",
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='29' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [29/29 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =     9.9282\n",
      "  eval_loss               =     1.8655\n",
      "  eval_perplexity         =     6.4594\n",
      "  eval_runtime            = 0:00:34.83\n",
      "  eval_samples_per_second =      6.488\n",
      "  eval_steps_per_second   =      0.833\n",
      "Evaluation Metrics: {'eval_loss': 1.865541696548462, 'eval_runtime': 34.831, 'eval_samples_per_second': 6.488, 'eval_steps_per_second': 0.833, 'epoch': 9.928220255653883, 'eval_perplexity': 6.4594340324401855}\n",
      "Fine-tuning finished.\n",
      "Best LoRA adapter saved in: ./smoltalk-finetuned-deepseek-lora\n",
      "Best Eval Loss: 1.8655\n",
      "Best Eval Perplexity: 6.4594340324401855\n"
     ]
    }
   ],
   "source": [
    "if len(tokenized_train_dataset) > 0:\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    train_result = trainer.train()\n",
    "\n",
    "    # Save the LoRA Adapter\n",
    "    print(f\"Saving best LoRA adapter to {output_dir}\")\n",
    "    trainer.save_model(output_dir)\n",
    "\n",
    "    # Log final metrics\n",
    "    metrics = train_result.metrics\n",
    "\n",
    "    # Calculate perplexity for training set\n",
    "    try:\n",
    "        train_perplexity = torch.exp(torch.tensor(metrics[\"train_loss\"])).item()\n",
    "        metrics[\"train_perplexity\"] = train_perplexity\n",
    "    except KeyError:\n",
    "        print(\"Could not calculate train perplexity (train_loss not found in metrics).\")\n",
    "    except OverflowError:\n",
    "         metrics[\"train_perplexity\"] = float(\"inf\")\n",
    "\n",
    "\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "\n",
    "    # Explicitly evaluate on the evaluation set and log metrics\n",
    "    if len(tokenized_eval_dataset) > 0:\n",
    "        print(\"Running final evaluation...\")\n",
    "        eval_metrics = trainer.evaluate()\n",
    "        # Calculate perplexity for evaluation set\n",
    "        try:\n",
    "            eval_perplexity = torch.exp(torch.tensor(eval_metrics[\"eval_loss\"])).item()\n",
    "            eval_metrics[\"eval_perplexity\"] = eval_perplexity\n",
    "        except OverflowError:\n",
    "            eval_metrics[\"eval_perplexity\"] = float(\"inf\")\n",
    "\n",
    "        trainer.log_metrics(\"eval\", eval_metrics)\n",
    "        trainer.save_metrics(\"eval\", eval_metrics)\n",
    "        print(f\"Evaluation Metrics: {eval_metrics}\")\n",
    "\n",
    "\n",
    "    print(\"Fine-tuning finished.\")\n",
    "    print(f\"Best LoRA adapter saved in: {output_dir}\")\n",
    "    if \"eval_loss\" in eval_metrics:\n",
    "        print(f\"Best Eval Loss: {eval_metrics['eval_loss']:.4f}\")\n",
    "        print(f\"Best Eval Perplexity: {eval_metrics.get('eval_perplexity', 'N/A')}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping training as the processed training dataset is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c95d8e5",
   "metadata": {},
   "source": [
    "## Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f22866",
   "metadata": {},
   "source": [
    "#### Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path where trained adapter was saved\n",
    "adapter_path = f\"./smoltalk-finetuned-deepseek-lora\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fffd88",
   "metadata": {},
   "source": [
    "#### Load LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97fcb497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LoRA adapter from: ./smoltalk-finetuned-deepseek-lora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yukse\\Desktop\\Yuksel\\Yucas\\LLM\\Lib\\site-packages\\peft\\tuners\\lora\\bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./smoltalk-finetuned-merged\\\\tokenizer_config.json',\n",
       " './smoltalk-finetuned-merged\\\\special_tokens_map.json',\n",
       " './smoltalk-finetuned-merged\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
    "\n",
    "# Merge the adapter layers into the base model for inference\n",
    "merged_model = PeftModel.from_pretrained(model, adapter_path)\n",
    "\n",
    "# If you want to merge explicitly and save a standalone model \n",
    "merged_model = merged_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"./smoltalk-finetuned-merged\")\n",
    "tokenizer.save_pretrained(\"./smoltalk-finetuned-merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e645ac4",
   "metadata": {},
   "source": [
    "#### Set Model to Evaluation Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96bbff7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model set to evaluation mode.\n"
     ]
    }
   ],
   "source": [
    "merged_model.eval()\n",
    "print(\"Model set to evaluation mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d1fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Inference Function \n",
    "def generate_response(chat_history):\n",
    "    \"\"\"\n",
    "    Generates a response based on the provided chat history using the fine-tuned model.\n",
    "    Args:\n",
    "        chat_history (list): A list of dictionaries, e.g.,\n",
    "                             [{'role': 'user', 'content': 'Hello!'}, ...]\n",
    "    Returns:\n",
    "        str: The generated response content.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Apply the chat template for inference\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            chat_history,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "\n",
    "        # Tokenize the formatted prompt\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(merged_model.device) # Move inputs to model's device\n",
    "\n",
    "        print(\"Generating...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Generate response\n",
    "        with torch.no_grad(): \n",
    "            outputs = merged_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,          \n",
    "                pad_token_id=tokenizer.pad_token_id if tokenizer.pad_token_id is not None else tokenizer.eos_token_id, # Use pad or eos\n",
    "                eos_token_id=tokenizer.eos_token_id, # Stop generation when EOS is produced\n",
    "                do_sample=True,             \n",
    "                temperature=0.7,            # Controls randomness\n",
    "                top_k=50,                   # Consider top K tokens for sampling\n",
    "                top_p=0.9,                  # Nucleus sampling\n",
    "                repetition_penalty=1.2      # Penalize repeating tokens slightly\n",
    "            )\n",
    "        end_time = time.time()\n",
    "        print(f\"Generation took {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "        # Decode only the newly generated tokens\n",
    "        input_length = inputs.input_ids.shape[1]\n",
    "        generated_tokens = outputs[0][input_length:]\n",
    "        response = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        return response.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during generation: {e}\")\n",
    "        return \"[Error generating response]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc2e846",
   "metadata": {},
   "source": [
    "#### Test with Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a94f0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "Generation took 4.91 seconds.\n",
      "\n",
      "User: Hi there, how are you today?\n",
      "Assistant: Alright, the user is asking me how I'm doing. I should respond in a friendly and open manner. Maybe start with a smiley to keep it light.\n",
      "\n",
      "I want to let them know I'm still active, which shows interest. It's important to mention that I can assist with math problems since that's my strength. I'll add something about looking forward to their questions to keep the conversation going.\n",
      "</think>\n",
      "\n",
      "Hello! I'm just a Deep Thinking AI, so I don't have feelings, but I'm here and ready to help you with any math-related queries! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# Example 1:\n",
    "chat1 = [{'role': 'user', 'content': 'Hi there, how are you today?'}]\n",
    "response1 = generate_response(chat1)\n",
    "print(f\"\\nUser: {chat1[-1]['content']}\")\n",
    "print(f\"Assistant: {response1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af928f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating...\n",
      "Generation took 14.08 seconds.\n",
      "\n",
      "User: Can you tell me a fun fact?\n",
      "Assistant: Okay, the user sent \"Can you tell me a fun fact?\" So they want something interesting and entertaining. I should respond by offering different types of fun factsâ€”maybe math or science ones because those often get more attention.\n",
      "\n",
      "I remember hearing about pi memorization competitions. That's pretty cool and educational. Plus, it's a common topic that everyone knows, which makes it relatable.\n",
      "\n",
      "Another idea is space exploration. It's not just about winning awards; people often talk about learning from such events, especially when it brings them close to their heroes. It adds a personal touch too.\n",
      "\n",
      "Maybe also include a joke to keep it light-hearted. Jokes are always funny and easy to understand. They can use humor without being mean, which might be more engaging for some users.\n",
      "\n",
      "I need to make sure my response is friendly and inviting, letting them know I'm happy to share more if they're interested. Keeping it concise but informative will meet their needs.\n",
      "</think>\n",
      "\n",
      "Of course! Here's a fun fact for you: Did you know that pi (Ï€) is one of the most famous mathematical constants, representing the ratio of a circle's circumference to its diameter? The first few digits of pi are 3.1415926535..., and memorizing pi has become a popular challenge, with world records setting records for memorizing thousands of digits in a row!\n",
      "\n",
      "Did you know? Space exploration is a field where people often learn a lot from wins, no matter how smallâ€”or even from watching movies or TV shows? And sometimes, people just laugh at themselves for forgetting something like the largest planet in our solar system?\n",
      "\n",
      "And what else? A joke about why we should never forget... well, why did the mathematician go to bed? To solve his problems! ðŸ˜„\n"
     ]
    }
   ],
   "source": [
    "# Example 2:\n",
    "chat2 = chat1 + [{'role': 'assistant', 'content': response1},\n",
    "                 {'role': 'user', 'content': 'Can you tell me a fun fact?'}]\n",
    "response2 = generate_response(chat2)\n",
    "print(f\"\\nUser: {chat2[-1]['content']}\")\n",
    "print(f\"Assistant: {response2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c497406c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Interactive Chat (type 'quit' to exit) ---\n",
      "Generating...\n",
      "Generation took 8.02 seconds.\n",
      "Assistant: I need to determine the correct number by following these steps. First, I'll select any number from 1 to 10. After selecting, I will write it down and then try again until I get an accurate answer.\n",
      "\n",
      "Next, if you're unsure of my selection or want me to verify, we can proceed togetherively.\n",
      "</think>\n",
      "\n",
      "**Step-by-step Explanation:**\n",
      "\n",
      "To find a randomly selected number between **1-10**, follow these easy instructions:\n",
      "\n",
      "---\n",
      "\n",
      "### Step 1 â€“ Select Your Number\n",
      "1. Choose any number from **1** to **10** (e.g., 7).\n",
      "2. Write your chosen number in the provided space below.\n",
      "\n",
      "---\n",
      "\n",
      "### Step 2 â€“ Verify the Selection (Optional)\n",
      "If you are uncertain about which number was selected:\n",
      "1. Click on \"Re-spin\" when desired.\n",
      "2. The system will attempt to simulate re-selecting based on your initial choice, giving you confidence in its accuracy.\n",
      "\n",
      "---\n",
      "\n",
      "By completing these steps, you've successfully identified the randomly selected number!\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Interactive Chat Loop \n",
    "print(\"\\n--- Interactive Chat (type 'quit' to exit) ---\")\n",
    "chat_history = []\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == 'quit':\n",
    "        break\n",
    "    chat_history.append({'role': 'user', 'content': user_input})\n",
    "    assistant_response = generate_response(chat_history)\n",
    "    print(f\"Assistant: {assistant_response}\")\n",
    "    chat_history.append({'role': 'assistant', 'content': assistant_response})\n",
    "    # Trim history to prevent excessive length\n",
    "    if len(chat_history) > 20:\n",
    "        chat_history = chat_history[-20:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
